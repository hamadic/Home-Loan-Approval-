
import numpy as np
import pandas as pd
import os
import seaborn as sns
import matplotlib.pyplot as plt
import os
os.getcwd()

#libraries for preprocessing
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

#import performance metric functions
from sklearn.metrics import confusion_matrix as cm
from sklearn.metrics import precision_score, recall_score

# 1- Import Dataframe
os.chdir(r'C:\Users\ramsey\Desktop\Metro College\Python Capstone')
train = pd.read_csv("Train_Loan_Home.csv")#### the training data
test = pd.read_csv("Test_Loan_Home.csv")

# Add col 'source' to datasets
train['source'] = 'train'
test['source']='test'
data=pd.concat([train, test], ignore_index = True, sort = True)

data.head(2)
print(train.shape, test.shape, data.shape)
data[data.source == 'test'].head(3)

f= data[data.Loan_Status.isnull()].head(2)

data.Loan_Status.unique()


data.apply(lambda x: len(x.unique()))

data.dtypes
    
data.count()
    
#### Cross Tabing 


barcredit = pd.crosstab(data['Credit_History'], data['Gender'])
barcredit.plot(kind='bar', stacked = True, color=['red', 'blue'], grid = False)


bararea = pd.crosstab([data['Property_Area']], data['Loan_Status'])
bararea.plot(kind='bar', stacked = False, color=['green', 'blue'], grid = False)



#### Find Categories 
data['Married'].unique()
data ['Gender'].unique()
data ['Dependents'].unique()
data ['Education'].unique()
data ['Self_Employed'].unique()
data ['Credit_History'].unique()
data ['Property_Area'].unique()
data ['Loan_Status'].unique()
data['Loan_Amount_Term'].unique()
#Missing Values 
data['Gender'].isnull().sum().sum() # 24 
data['Married'].isnull().sum().sum() #3
data['Dependents'].isnull().sum().sum() #25
data['Education'].isnull().sum().sum() # 0
data['Self_Employed'].isnull().sum().sum() #55
data['ApplicantIncome'].isnull().sum().sum() #0
data['LoanAmount'].isnull().sum().sum() #27
data['Loan_Amount_Term'].isnull().sum().sum() #20
data['Credit_History'].isnull().sum().sum() #79
data['Property_Area'].isnull().sum().sum() # 0
data['Loan_Status'].isnull().sum().sum() #367
data['Loan_ID'].isnull().sum().sum() #0
data['CoapplicantIncome'].isnull().sum().sum() #0



### Dealing with Missing Values

data['Gender']=data['Gender'].fillna('Unknown')
data['Married'] =data['Married'].fillna('Unknown') 
data['Dependents'] =data['Dependents'].fillna('0')
data['Dependents'].mode()
data['Dependents'].unique()
data['Self_Employed'] =data['Self_Employed'].fillna('Unknown')
data['Loan_Amount_Term'] =data['Loan_Amount_Term'].fillna(data['Loan_Amount_Term'].median()) 
data['Loan_Amount_Term'].mode()
data['Loan_Amount_Term'].mean()
data['Loan_Amount_Term'].median()
data['Loan_Amount_Term'].hist()
data['Loan_Amount_Term'].unique()
data['Loan_Amount_Term'].describe()
data['Credit_History'] =data['Credit_History'].fillna(1)
data['Credit_History'].mode()
### replacing continous features with the mean for missing values 

data['LoanAmount'].describe()
data['LoanAmount']=data['LoanAmount'].fillna(data['LoanAmount'].mean())
data['LoanAmount'].describe()
data['LoanAmount'].isnull().sum().sum()

data.isnull().sum().sum()
data.boxplot(column  = 'LoanAmount')


# Ensure no missing values
data['Married'].unique()
data ['Gender'].unique()
data ['Dependents'].unique()
data ['Education'].unique()
data ['Self_Employed'].unique()
data ['Credit_History'].unique()
data ['Property_Area'].unique()
data ['Loan_Status'].unique()
data['Loan_Amount_Term'].unique()

### visualization 
#ApplicantIncome        int64
data['ApplicantIncome'].hist(bins=10)
plt.title('Histogram of ApplicantIncome')
plt.show()

data.boxplot(column  = 'ApplicantIncome')# Need binning to deal with outliers


#CoapplicantIncome    float64
data['CoapplicantIncome'].hist(bins=10)
plt.title('Histogram of CoapplicantIncome')
plt.show()
data.boxplot(column  = 'CoapplicantIncome')# Need binning to deal with outliers

#### Summing the total amount Income (Applicant Income + Coapplicant Income)

data['Total_Income'] = data.ApplicantIncome + data.CoapplicantIncome

#####

data['Total_Income'].describe()
median(data.Total_Income)

data['Total_Income'].hist(bins=10)
plt.ylabel('Frequency')
plt.xlabel('Total Income: Applicant + Coapplicant')

data.boxplot(column  = 'Total_Income', by='Loan_Status')

Q1 = data['Total_Income'].quantile(0.25)
Q3 = data['Total_Income'].quantile(0.75)
IQR = Q3 - Q1
print(IQR)
Outl=Q3+1.5*IQR # 12021.0
Q9 = data['Total_Income'].quantile(0.90) # 10890.0

# binning Total Income 
bins = [1442, 5000, 12000, 81000]
labels = ['0-5000', '5001-12000', '12001-81000']
data['Total_Income_Coded'] = pd.cut(data.Total_Income, bins=[1442, 5000, 12000, 81000], labels = labels, include_lowest=True)
data['Total_Income_Coded'].describe()
barincome = pd.crosstab(data['Total_Income_Coded'], data['Loan_Status'])
barincome.plot(kind='bar', stacked = 1, color=['red', 'blue'], grid = False)
plt.ylabel("Frequency")
plt.xlabel('Total_Income_Group')
plt.title('Total_Income_Group')
plt.show()


#Credit_History       float64
barcredit = pd.crosstab(data['Credit_History'], data['Loan_Status'])
barcredit.plot(kind='bar', stacked = 1, color=['red', 'blue'], grid = False)
plt.ylabel("Frequency")
plt.xlabel('Credit History')
plt.title('Credit History')
plt.show()

#Dependents            object
bardep = pd.crosstab(data['Dependents'], data['Loan_Status'])
barcdep.plot(kind='bar', stacked = 1, color=['red', 'blue'], grid = False)
plt.ylabel("Frequency")
plt.xlabel('Dependents')
plt.title('Dependents')
plt.show()

#Education             object

baredu = pd.crosstab(data['Education'], data['Loan_Status'])
baredu.plot(kind='bar', stacked = 0, color=['red', 'blue'], grid = False)
plt.ylabel("Frequency")
plt.xlabel('Education')
plt.title('Education')
plt.show()

#Gender                object
barGen = pd.crosstab(data['Gender'], data['Loan_Status'])
barGen.plot(kind='bar', stacked = 1, color=['red', 'blue'], grid = False)
plt.ylabel("Frequency")
plt.xlabel('Gender')
plt.title('Gender')
plt.show()

#LoanAmount           float64
data['LoanAmount'].hist()
data.boxplot(column  = 'LoanAmount')# I will use binning to deal with outliers

data['LoanAmount'].describe()

data.boxplot(column = 'Loan_Amount_Term')
data['LoanAmount'].hist(bins=5)
plt.title('Histogram of Loan Amount')
plt.show()

data.dtypes
data.boxplot(column  = 'LoanAmount', by = 'Gender')

Q1 = data['LoanAmount'].quantile(0.25)
Q3 = data['LoanAmount'].quantile(0.75)
IQR = Q3 - Q1
print(IQR)
Outl=Q3+1.5*IQR # 255.0
top10%= data['LoanAmount'].quantile(0.90) # 212.0

# binning LoanAmount
Loanbins = [0, 255, 700]
Loanlabels = ['<225', '>=225']
data['LoanAmount_coded'] = pd.cut(data.LoanAmount, bins=[0, 225, 700], labels = Loanlabels, include_lowest=True)
data['LoanAmount_coded'].describe()
barLoanAmount = pd.crosstab(data['LoanAmount_coded'], data['Loan_Status'])
barLoanAmount.plot(kind='bar', stacked = 0, color=['red', 'blue'], grid = False)
plt.ylabel("Frequency")
plt.xlabel('LoanAmount_coded')
plt.title('LoanAmount')
plt.show()
data['LoanAmount_coded'].unique()
data['LoanAmount_coded'].isnull().sum().sum()

#Loan_Amount_Term     float64
barLoanAmountTerm = pd.crosstab(data['Loan_Amount_Term'], data['Loan_Status'])
barLoanAmountTerm.plot(kind='bar', stacked = 0, color=['red', 'blue'], grid = False)
plt.ylabel("Frequency")
plt.xlabel('Loan_Amount_Term')
plt.title('Loan_Amount_Term')
plt.show()

Loan_Amount_Term_bins = [1,359, 480]
Loan_Amount_Term_labels = {"<360", ">=360"}              
data['Loan_Amount_Term_Coded'] = pd.cut(data.Loan_Amount_Term, bins=[12, 359, 480], labels = Loan_Amount_Term_labels, include_lowest=True)
data['Loan_Amount_Term_Coded'].describe()

#Loan_Status           object
data['Loan_Status'].value_counts().plot(kind='bar')
plt.grid(axis='y', alpha=0.75)
plt.ylabel('Frequency')
plt.xlabel('Loan_Status')
plt.title('Loan_Status')
plt.show()

#Married               object
barLoanAmount = pd.crosstab(data['Married'], data['Loan_Status'])
barLoanAmount.plot(kind='bar', stacked = 0, color=['red', 'blue'], grid = False)
plt.ylabel("Frequency")
plt.xlabel('Married')
plt.title('Married')
plt.show()

#Property_Area         object
barLoanAmount = pd.crosstab(data['Property_Area'], data['Loan_Status'])
barLoanAmount.plot(kind='bar', stacked = 0, color=['red', 'blue'], grid = False)
plt.ylabel("Frequency")
plt.xlabel('Property_Area')
plt.title('Property_Area')
plt.show()

#Self_Employed         object
barLoanAmount = pd.crosstab(data['Self_Employed'], data['Loan_Status'])
barLoanAmount.plot(kind='bar', stacked = 0, color=['red', 'blue'], grid = False)
plt.ylabel("Frequency")
plt.xlabel('Self_Employed')
plt.title('Self_Employed')
plt.show()



###### Find Correlation matrix

import seaborn as sns
corr = data.corr()
sns.heatmap(corr, 
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)




##data discription 
from collections import Counter 
Counter(data['Education'])
Counter(data['Gender'])
Counter(data['Loan_Amount_Term'])
Counter(data['Loan_Status'])
Counter(data['Married'])
Counter(data['Property_Area'])
Counter(data['Self_Employed'])
Counter(data['Loan_ID'])
Counter(data['Credit_History'])
Counter(data['Dependents'])
Counter(data['LoanAmount'])

## Rearrange Columns in dataset(move target variable to the end)
# rearrange columns 
data.columns

df=data.loc[:,['Loan_ID','source','Total_Income', 'Loan_Amount_Term_Coded','Total_Income_Coded','LoanAmount_coded','ApplicantIncome','LoanAmount','Credit_History','Dependents','Education','Gender','LoanAmount','Loan_Amount_Term','Married', 'Property_Area', 'Self_Employed','Loan_Status']]

import statsmodels.formula.api as smf

result1 = smf.logit('Loan_Status_Coded ~ Loan_Amount_Term_Coded + Total_Income_Coded + LoanAmount_coded + C(ApplicantIncome) + C(LoanAmount) + C(Credit_History) + C(Dependents) + C(Education) + C(Gender) + C(LoanAmount) + C(Loan_Amount_Term) + C(Married) + C(Property_Area) + C(Self_Employed), data=df).fit()
print(result1.summary())
plt.rc('figure', figsize=(12, 7))
plt.text(0.01, 0.05, str(result1.summary()), {'fontsize': 10}, fontproperties = 'monospace') # approach improved by OP -> monospace!
plt.axis('off')
plt.tight_layout()
plt.savefig('output.png')### R-Squared .31 - for only the training set




#########################################################################

###### Baseline Model  

def coding(col, codeDict):
  colCoded = pd.Series(col, copy=True)
  for key, value in codeDict.items():
    colCoded.replace(key, value, inplace=True)
  return colCoded

Loan_Status_bins = [0,1,2]
Loan_Status_labels = {"Approved", "Not Approved"}              
df['Loan_Status_Coded'] = coding(df['Loan_Status'], {'Y': 0, 'N': 1})

ata = pd.get_dummies(df, columns=['source','Total_Income', 'Loan_Amount_Term_Coded','Total_Income_Coded','LoanAmount_coded','ApplicantIncome','LoanAmount','Credit_History','Dependents','Education','Gender','LoanAmount','Loan_Amount_Term','Married', 'Property_Area', 'Self_Employed', 'Loan_Status_Coded'])




### spliting the data again into the original settings of traing set and test set 
trainset = ata.loc[data['source']=="train"]

testset = ata.loc[data['source']=="test"]

# #extract independent variables from the trainset
list(trainset.columns)
list(testset.columns)

X_train = trainset.iloc[:, 0:2574]
X_test = testset.iloc[:, 0:2574]


#extract dependent variable from the trainset
y_train = trainset.iloc[:,-1]
y_test = testset.iloc[:,-1]##### As all values are missing, all have been coded as 0 ... The target is to predict y_test true values 

###Baseline Model (Nive Byes)
from sklearn.naive_bayes import GaussianNB
NB = GaussianNB()
NB.fit(X_train, y_train)

y_pred=NB.predict(X_test)
y_pred_prob= NB.predict_proba(X_test)

NB.score(X_test, y_test) # 0.43

from sklearn.linear_model import LinearRegression



################################## KNN Model model performance

from sklearn.neighbors import KNeighborsClassifier
from sklearn.cross_validation import cross_val_score
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import accuracy_score

classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
KNN_model = classifier.fit(X_train, y_train)
KNN_Score = classifier.score(X_train, y_train)##### KNN score 0.81 
KNN_y_pred = classifier.predict(X_test)# Predicition of y_test
KNN_y_pred_proba = classifier.predict_proba(X_test)# Predicition of y_test


X = np.concatenate((X_train, X_test), axis=0)
y = np.concatenate((y_train, KNN_y_pred), axis=None)### replacing the missing values with the predicted values of KNN model 

from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.2, random_state = 0) 

from sklearn.neighbors import KNeighborsClassifier
from sklearn.cross_validation import cross_val_score
from sklearn.grid_search import GridSearchCV


classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier.fit(X_train, y_train)
KNN_y = classifier.score(X_test,y_test)### 0.82
y_pred = classifier.predict(X_test)# Predicting the Test set results

knn = KNeighborsClassifier()

parameters = {'n_neighbors':[4,5,6,7],
              'leaf_size':[1,3,5],
              'algorithm':['auto', 'kd_tree'],
              'n_jobs':[-1]}


model = GridSearchCV(knn, param_grid=parameters)
model.fit(X_train,y_train.ravel())
model.score(X_train,y_train.ravel())###0.845
# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
KNN_cm = confusion_matrix(y_test, y_pred)

from sklearn.model_selection import cross_val_score
Accuracies_scores = cross_val_score(model, X_train, y_train, cv=2)

##################################3 logistic Regression 

from sklearn.linear_model import LogisticRegression
regressor = LogisticRegression()
Logit = regressor.fit(X_train, y_train)
Logit_Score = regressor.score(X_train, y_train)##### logit score 0.81 
Logit_y_pred = regressor.predict(X_test)# Predicition of y_test
Logit_y_pred_proba = regressor.predict_proba(X_test)# Predicition of y_test

Counter(Logit_y_pred)


X = np.concatenate((X_train, X_test), axis=0)
y = np.concatenate((y_train, Logit_y_pred), axis=None)### replacing the missing values with the predicted values of KNN model 

from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.2, random_state = 0) 
Logit = regressor.fit(X_train, y_train)
Logit_Score = regressor.score(X_test, y_test)##### logit score 0.83 
Logit_y_pred = regressor.predict(X_test)# Predicition of y_test
Logit_y_pred_proba = regressor.predict_proba(X_test)# Predicition of y_test

from sklearn.metrics import confusion_matrix
logit_cm = confusion_matrix(y_test, Logit_y_pred)

from sklearn.model_selection import cross_val_score
Accuracies_scores = cross_val_score(Logit, X_train, y_train, cv=5)

# Gridsearch for Logistics  
from sklearn import linear_model, datasets
logistic = linear_model.LogisticRegression()
penalty = ['l1', 'l2']
C = np.logspace(0, 4, 10)
hyperparameters = dict(C=C, penalty=penalty)
clf = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)
best_model = clf.fit(X_train, y_train.ravel())
best_model.score(X_train,y_train.ravel())### 0.81
print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])
print('Best C:', best_model.best_estimator_.get_params()['C'])
best_model.predict()

#################################

############################## Random Forest model 

# Fitting Random Forest Classification to the Training set
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
RF_model = classifier.fit(X_train, y_train)
RF_Score = classifier.score(X_train, y_train)##### Random Forest score 0.94 
RF_y_pred = classifier.predict(X_test)# Predicition of y_test
RF_y_pred_proba = classifier.predict_proba(X_test)# Predicition of y_test

Counter(RF_y_pred)


X = np.concatenate((X_train, X_test), axis=0)
y = np.concatenate((y_train, RF_y_pred), axis=None)### replacing the missing values with the predicted values of KNN model 


from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.2, random_state = 0) 
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
RF_model = classifier.fit(X_train, y_train)
RF_Score = classifier.score(X_test, y_test)##### Random Forest score 0.87 
RF_y_pred = classifier.predict(X_test)# Predicition of y_test
RF_y_pred_proba = classifier.predict_proba(X_test)# Predicition of y_test


from sklearn.metrics import confusion_matrix
RF_cm = confusion_matrix(y_test, RF_y_pred)

from sklearn.model_selection import cross_val_score
Accuracies_scores = cross_val_score(classifier, X_train, y_train, cv=5)

###################################### 

################################ Perceptron Model 

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_digits
from sklearn.linear_model import Perceptron

classifier = Perceptron(n_iter=300, eta0 =0.25, random_state=0)
Perceptron_model = classifier.fit(X_train, y_train)
Perceptron_Score = classifier.score(X_train, y_train)##### Perceptron score 0.64 
Perceptron_y_pred = classifier.predict(X_test)# Predicition of y_test
print(classifier.intercept_, classifier.coef_)

Counter(Perceptron_y_pred)

X = np.concatenate((X_train, X_test), axis=0)
y = np.concatenate((y_train, Perceptron_y_pred), axis=None)### replacing the missing values with the predicted values of KNN model 

X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)
classifier = Perceptron(n_iter=300, eta0 =0.25, random_state=0)
Perceptron_model = classifier.fit(X_train, y_train)
Perceptron_Score = classifier.score(X_test, y_test)##### Perceptron score 0.78 
Perceptron_y_pred = classifier.predict(X_test)# Predicition of y_test

from sklearn.metrics import confusion_matrix
Perceptron_cm = confusion_matrix(y_test, Perceptron_y_pred)


from sklearn.model_selection import cross_val_score
Accuracies_scores = cross_val_score(Perceptron_model, X_train, y_train.ravel(), cv=5)

#####################################################################################
####################################### SVM with GridSearch 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

from sklearn.svm import SVC
classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(X_train, y_train)
classifier.score(X_train, y_train)###Kernel = linear #####0.84

classifier = SVC(kernel = 'rbf', random_state = 0)
classifier.fit(X_train, y_train)
classifier.score(X_train, y_train)###Kernel = rbf #####0.84


from sklearn.model_selection import GridSearchCV
params = {
            'C':[0.1, 1, 10],
            'kernel':['linear', 'poly', 'rbf'],
            'degree':[2,3,4],
            'gamma':[0.001, 0.01, 0.1],
            'tol':[0.001, 0.01, 0.1]        
        }



from sklearn.svm import SVC
clf = SVC()
svm = GridSearchCV(clf,params)
svm.fit(X_train,y_train.ravel())
svm.best_score_#####0.84
svm.best_params_### {'C': 0.1, 'degree': 2, 'gamma': 0.001, 'kernel': 'linear', 'tol': 0.001}

svm_best = SVC(C = 10, degree= 2, gamma = 0.0001, kernel= 'rbf')
svm_best.fit(X_train,y_train)
y_pred = svm_best.predict(X_test)
svm_best.score(X_test,y_test)##0.75











